{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28","authorship_tag":"ABX9TyMJ5XV/uPPzMoPTnzerZjHo"},"accelerator":"TPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12884358,"sourceType":"datasetVersion","datasetId":8151534}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#from google.colab import drive\nimport requests\nimport re\nimport os\nDRIVE_PATH = \"/content/drive/MyDrive/LLMProject/\"\nFILENAME = \"scifi_llm_dataset.txt\"\n#OUTPUT_FILE = os.path.join(DRIVE_PATH, FILENAME)\nOUTPUT_FILE  = \"/kaggle/input/modeldata/scifi_llm_dataset.txt\"\n#drive.mount('/content/drive')","metadata":{"id":"EEb1LZWRZMYI","executionInfo":{"status":"ok","timestamp":1756283138003,"user_tz":-180,"elapsed":963,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"db3300fb-1aed-435c-ce71-cbe8929d144c","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.736571Z","iopub.execute_input":"2025-08-28T13:46:47.737226Z","iopub.status.idle":"2025-08-28T13:46:47.741194Z","shell.execute_reply.started":"2025-08-28T13:46:47.737196Z","shell.execute_reply":"2025-08-28T13:46:47.740229Z"}},"outputs":[],"execution_count":77},{"cell_type":"code","source":"\n#Desired Sci-Fi authors for training the llm\n\nBOOK_IDS = {\n     # H.G. Wells\n    '35': 'The Time Machine by H. G. Wells',\n    '36': 'The War of the Worlds by H. G. Wells',\n    '5230': 'The Invisible Man by H. G. Wells',\n    '159': 'The Island of Doctor Moreau by H. G. Wells',\n    '1013': 'The First Men in the Moon by H. G. Wells',\n\n    # Jules Verne\n    '32': 'Journey to the Center of the Earth by Jules Verne',\n    '105': 'From the Earth to the Moon by Jules Verne',\n    '164': 'Twenty Thousand Leagues under the Sea by Jules Verne',\n    '1268': 'The Mysterious Island by Jules Verne',\n    '83': 'Around the Moon by Jules Verne',\n\n    # Edgar Rice Burroughs\n    '64317': 'A Princess of Mars by Edgar Rice Burroughs',\n    '68': 'The Gods of Mars by Edgar Rice Burroughs',\n\n    # Arthur Conan Doyle\n    '1952': 'The Lost World by Arthur Conan Doyle',\n    '22357': 'The Poison Belt by Arthur Conan Doyle',\n\n    # Mary Shelley & R.L. Stevenson (Sci-Fi Origins)\n    '84': 'Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley',\n    '16': 'The Strange Case of Dr. Jekyll and Mr. Hyde by Robert Louis Stevenson',\n\n    # Other Influential Classics\n    '97': 'Flatland: A Romance of Many Dimensions by Edwin Abbott Abbott',\n    '21970': 'The Scarlet Plague by Jack London',\n    '32032': 'Second Variety by Philip K. Dick',\n    '1513': 'Moby Dick; Or, The Whale by Herman Melville', # Proto-sci-fi\n    '73723': 'The Machine Stops by E. M. Forster',\n    '19141': 'Edison\\'s Conquest of Mars by Garrett Putnam Serviss',\n    '32338': 'The Metal Monster by Abraham Merritt',\n    '18452': 'The Hampdenshire Wonder by J. D. Beresford',\n    '521': 'The Last Man by Mary Wollstonecraft Shelley'\n}\ndef word_count(text):\n  return len(re.findall(r'\\b\\w+\\b', text.lower()))\n","metadata":{"id":"oAOqMnyEagHY","executionInfo":{"status":"ok","timestamp":1756283138004,"user_tz":-180,"elapsed":3,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.742293Z","iopub.execute_input":"2025-08-28T13:46:47.742582Z","iopub.status.idle":"2025-08-28T13:46:47.759880Z","shell.execute_reply.started":"2025-08-28T13:46:47.742557Z","shell.execute_reply":"2025-08-28T13:46:47.759050Z"}},"outputs":[],"execution_count":78},{"cell_type":"code","source":"collected_texts = []\ntotal_words = 0\nbook_count = 0\n\nprint(\"Starting direct download of selected books...\")\nif os.path.exists(OUTPUT_FILE):\n    print(f\"Output file '{OUTPUT_FILE}' already exists. Skipping download.\")\nelse:\n  for book_id, title in BOOK_IDS.items():\n      url = f\"https://www.gutenberg.org/ebooks/{book_id}.txt.utf-8\"\n\n      try:\n          # Download the book's text\n          response = requests.get(url)\n          response.raise_for_status()\n          text = response.text\n\n          #Clean the Gutenberg header and footer\n          start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n          end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n\n          start_pos = text.find(start_marker)\n          if start_pos != -1:\n              start_pos = text.find(\"\\n\", start_pos) + 1\n              text = text[start_pos:]\n\n          end_pos = text.rfind(end_marker) # Use rfind to find the last occurrence\n          if end_pos != -1:\n              text = text[:end_pos]\n\n          cleaned_text = text.strip()\n          collected_texts.append(cleaned_text)\n\n          current_words = word_count(cleaned_text)\n          total_words += current_words\n          book_count += 1\n          print(f\"Downloaded '{title}' | Words: {current_words:,}\")\n\n      except requests.exceptions.RequestException as e:\n          print(f\"Failed to download book ID {book_id}: {e}\")\n\n  print(\"\\nFinished downloading all books.\")\n  if book_count > 0:\n      print(f\"Final word count: {total_words:,} from {book_count} books.\")\n      print(f\"Saving all collected text to '{OUTPUT_FILE}'...\")\n      with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n          f.write(\"\\n\\n\".join(collected_texts))\n      print(\"Done\")\n  else:\n      print(\"No books were downloaded.\")","metadata":{"id":"V0Cy6yKurrnf","executionInfo":{"status":"ok","timestamp":1756283138010,"user_tz":-180,"elapsed":7,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"97d71194-13de-4982-93db-a00989ea9b0c","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.761534Z","iopub.execute_input":"2025-08-28T13:46:47.761743Z","iopub.status.idle":"2025-08-28T13:46:47.816058Z","shell.execute_reply.started":"2025-08-28T13:46:47.761718Z","shell.execute_reply":"2025-08-28T13:46:47.815331Z"}},"outputs":[{"name":"stdout","text":"Starting direct download of selected books...\nOutput file '/kaggle/input/modeldata/scifi_llm_dataset.txt' already exists. Skipping download.\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n    data = f.read()\ndata[:500]\n","metadata":{"id":"x0JFtEp7JQ1B","executionInfo":{"status":"ok","timestamp":1756283138057,"user_tz":-180,"elapsed":46,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"f6f1dd19-e063-4f26-bbce-6693a2e140f1","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.816912Z","iopub.execute_input":"2025-08-28T13:46:47.817114Z","iopub.status.idle":"2025-08-28T13:46:47.883005Z","shell.execute_reply.started":"2025-08-28T13:46:47.817092Z","shell.execute_reply":"2025-08-28T13:46:47.882302Z"}},"outputs":[{"execution_count":80,"output_type":"execute_result","data":{"text/plain":"'The Time Machine\\n\\nAn Invention\\n\\nby H. G. Wells\\n\\n\\nCONTENTS\\n\\n I Introduction\\n II The Machine\\n III The Time Traveller Returns\\n IV Time Travelling\\n V In the Golden Age\\n VI The Sunset of Mankind\\n VII A Sudden Shock\\n VIII Explanation\\n IX The Morlocks\\n X When Night Came\\n XI The Palace of Green Porcelain\\n XII In the Darkness\\n XIII The Trap of the White Sphinx\\n XIV The Further Vision\\n XV The Time Traveller’s Return\\n XVI After the Story\\n Epilogue\\n\\n\\n\\n\\n I.\\n Introduction\\n\\n\\nThe Time Traveller (for so it will '"},"metadata":{}}],"execution_count":80},{"cell_type":"code","source":"#count the number of times each pair of encoded bytes appears together in the data\ndef get_stats(ids, counts = None):\n  counts = {} if counts is None else counts\n  for pair in zip(ids,ids[1:]):\n    counts[pair] = counts.get(pair,0)+1\n  return counts\n","metadata":{"id":"wXuraDpFKgOX","executionInfo":{"status":"ok","timestamp":1756283138074,"user_tz":-180,"elapsed":14,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.884630Z","iopub.execute_input":"2025-08-28T13:46:47.884834Z","iopub.status.idle":"2025-08-28T13:46:47.888723Z","shell.execute_reply.started":"2025-08-28T13:46:47.884819Z","shell.execute_reply":"2025-08-28T13:46:47.887996Z"}},"outputs":[],"execution_count":81},{"cell_type":"code","source":"#in the list of ids replace all consecutive pairs with the new token idx\ndef merge(ids,pair,idx):\n  newids = []\n  i = 0\n  while i < len(ids):\n    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n      newids.append(idx)\n      i+=2\n    else:\n      newids.append(ids[i])\n      i+=1\n  return newids\nprint(merge([2,3,3,4,5,6],(3,4),333))","metadata":{"id":"frHNfjKMKinb","executionInfo":{"status":"ok","timestamp":1756283138080,"user_tz":-180,"elapsed":4,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"8e36148e-23af-49c0-cde7-079551fea5c3","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.889394Z","iopub.execute_input":"2025-08-28T13:46:47.889633Z","iopub.status.idle":"2025-08-28T13:46:47.904228Z","shell.execute_reply.started":"2025-08-28T13:46:47.889618Z","shell.execute_reply":"2025-08-28T13:46:47.903514Z"}},"outputs":[{"name":"stdout","text":"[2, 3, 333, 5, 6]\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"import regex as re\n#Some hyperparameters for the training of the tokenizer\nVOCAB_SIZE = 8256 # The number of tokens in our vocabulary\nNUM_MERGES = VOCAB_SIZE - 256 # 8000 merges to obtain the desired size of the vocab\n#I will use the split pattern from the gpt 4 as it is the most complete I could find\nGPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n","metadata":{"id":"M5RvrSNnUcVa","executionInfo":{"status":"ok","timestamp":1756283138102,"user_tz":-180,"elapsed":21,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.905088Z","iopub.execute_input":"2025-08-28T13:46:47.905291Z","iopub.status.idle":"2025-08-28T13:46:47.916196Z","shell.execute_reply.started":"2025-08-28T13:46:47.905277Z","shell.execute_reply":"2025-08-28T13:46:47.915495Z"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":"#This function trains the tokenizer\ndef train(text:str):\n  ids = []\n  text_split = re.findall(GPT4_SPLIT_PATTERN, text)\n\n  for chars in text_split:\n    byte_list  = list(chars.encode(\"utf-8\"))\n    ids.append(byte_list)\n\n  merges = {}\n  vocab = {idx: bytes([idx]) for idx in range(256)}\n\n  for i in range(NUM_MERGES):\n    stats = {}\n    for chars in ids:\n      get_stats(chars,stats)\n    if not stats:\n      break\n    pair = max(stats, key = stats.get)\n    idx = 256 + i\n    ids = [merge(chars, pair, idx) for chars in ids]\n    merges[pair] = idx\n    vocab[idx] = vocab[pair[0]] + vocab[pair[1]]\n  return vocab, merges\n","metadata":{"id":"SEXmU_MkVtlT","executionInfo":{"status":"ok","timestamp":1756283138130,"user_tz":-180,"elapsed":18,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.916836Z","iopub.execute_input":"2025-08-28T13:46:47.917063Z","iopub.status.idle":"2025-08-28T13:46:47.924176Z","shell.execute_reply.started":"2025-08-28T13:46:47.917044Z","shell.execute_reply":"2025-08-28T13:46:47.923681Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"def encode(text:str, merges):\n  tokens = list(text.encode(\"utf-8\"))\n  while len(tokens) >=2:\n    stats = get_stats(tokens)\n    pair = min(stats, key = lambda s: merges.get(s, float(\"inf\")))\n    if pair not in merges:\n      break\n    idx = merges[pair]\n    tokens = merge(tokens, pair, idx)\n  return tokens\ndef decode(ids, vocab):\n  # given ids (list of integers), return Python string\n  tokens = b\"\".join(vocab[idx] for idx in ids)\n  text = tokens.decode(\"utf-8\", errors=\"replace\") #replace inserts a special character for undecodable byte\n  return text\n\n\nvocab_dummy = {}\nmerges_dummy = {}\ndummy_text = \"This is a dummy text\"\nvocab_dummy, merges_dummy = train(\"This is a dummy mmm his is\")\nprint(decode(encode(dummy_text, merges_dummy), vocab_dummy))\n\nprint(encode(dummy_text, merges_dummy))\nprint(list(dummy_text.encode(\"utf-8\")))\nprint(vocab_dummy[265])","metadata":{"id":"nQjt6nVLY6ds","executionInfo":{"status":"ok","timestamp":1756283138132,"user_tz":-180,"elapsed":8,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"f3abe8df-eb8c-42c6-a2d0-13d9c1a437b1","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.924933Z","iopub.execute_input":"2025-08-28T13:46:47.925608Z","iopub.status.idle":"2025-08-28T13:46:47.937080Z","shell.execute_reply.started":"2025-08-28T13:46:47.925586Z","shell.execute_reply":"2025-08-28T13:46:47.936515Z"}},"outputs":[{"name":"stdout","text":"This is a dummy text\n[260, 259, 261, 265, 32, 116, 101, 120, 116]\n[84, 104, 105, 115, 32, 105, 115, 32, 97, 32, 100, 117, 109, 109, 121, 32, 116, 101, 120, 116]\nb' dummy'\n","output_type":"stream"}],"execution_count":85},{"cell_type":"markdown","source":"The base functions for the tokenizer are finished, now what is left is to create methods for loading and saving the tokens obtained during the training phase. After this I will start building the proper llm from scratch.","metadata":{"id":"J92NG0FagoXb"}},{"cell_type":"code","source":"import unicodedata\n\n#This function builds the vocabulary from the merges that we perform\ndef build_vocab(merges):\n  vocab = {idx: bytes([idx]) for idx in range(256)}\n  for (p0, p1), idx in merges.items():\n      vocab[idx] = vocab[p0] + vocab[p1]\n  return vocab\n\n#We will also define two functions such that we can better visualize our merges and vocab dictionaries\ndef replace_control_chars(s: str):\n    chars = []\n    for ch in s:\n        if unicodedata.category(ch)[0] != \"C\":\n            chars.append(ch)\n        else:\n            chars.append(f\"\\\\u{ord(ch):04x}\") # escape\n    return \"\".join(chars)\n\ndef render_token(t: bytes) -> str:\n    # pretty print a token, escaping control characters\n    s = t.decode('utf-8', errors='replace')\n    s = replace_control_chars(s)\n    return s","metadata":{"id":"OyVksGAOm90T","executionInfo":{"status":"ok","timestamp":1756283138133,"user_tz":-180,"elapsed":5,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.937759Z","iopub.execute_input":"2025-08-28T13:46:47.937994Z","iopub.status.idle":"2025-08-28T13:46:47.950252Z","shell.execute_reply.started":"2025-08-28T13:46:47.937979Z","shell.execute_reply":"2025-08-28T13:46:47.949510Z"}},"outputs":[],"execution_count":86},{"cell_type":"code","source":"def save(vocab_file, model_file, merges, vocab):\n    with open(model_file, 'w') as f:\n            # the merges dict\n            for idx1, idx2 in merges:\n                f.write(f\"{idx1} {idx2}\\n\")\n    inverted_merges = {idx: pair for pair, idx in merges.items()} # this is needed to find the children of each token that was newly created\n    with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n        for idx, token in vocab.items():\n            s = render_token(token)\n            # find the children of this token\n            if idx in inverted_merges:\n                # if this token has children show the merge\n                idx0, idx1 = inverted_merges[idx]\n                s0 = render_token(vocab[idx0])\n                s1 = render_token(vocab[idx1])\n                f.write(f\"[{s0}][{s1}] -> [{s}] {idx}\\n\")\n            else:\n                f.write(f\"[{s}] {idx}\\n\") # this case is for the first 256 tokens\ndef load(model_file):\n    merges = {}\n    idx = 256\n    with open(model_file, 'r', encoding=\"utf-8\") as f:\n        # read the merges\n        for line in f:\n            idx1, idx2 = map(int, line.split())\n            merges[(idx1, idx2)] = idx\n            idx += 1\n    vocab = build_vocab(merges)\n    return vocab, merges","metadata":{"id":"v7-ZjjLBg-Bg","executionInfo":{"status":"ok","timestamp":1756283138134,"user_tz":-180,"elapsed":5,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.952303Z","iopub.execute_input":"2025-08-28T13:46:47.952774Z","iopub.status.idle":"2025-08-28T13:46:47.965603Z","shell.execute_reply.started":"2025-08-28T13:46:47.952750Z","shell.execute_reply":"2025-08-28T13:46:47.964989Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"#In this cell I will test the tokenizer to see if it works correctly\nMODEL_FILE = \"/kaggle/input/modeldata/tokenizer.model\"\nVOCAB_FILE = \"/kaggle/input/modeldata/tokenizer.vocab\"\nif os.path.exists(MODEL_FILE) and os.path.exists(VOCAB_FILE):\n    print(f\"Model file '{MODEL_FILE}' and vocab file '{VOCAB_FILE}' already exist. Skipping training.\")\nelse:\n    print(\"Training my tokenizer...(hope it works)\")\n    vocab, merges = train(data)\n    print(\"Tokenizer trained (hopefully) \")\n    print(\"Saving tokenizer...\")\n    save(VOCAB_FILE, MODEL_FILE, merges, vocab)\n    print(\"Tokenizer saved\")\n\nprint(\"Loading tokenizer\")\nloaded_vocab, loaded_merges = load(MODEL_FILE)\nprint(\"Tokenizer loaded\")\n\nprint(\"Checking my tokenizer: \")\ndummy_sentence = \"I am confident my tokenizer works as it should\"\nprint(f\"Original sentence: {dummy_sentence}\")\n\nencoded_dummy = encode(dummy_sentence, loaded_merges)\nprint(f\"Encoded sentence: {encoded_dummy}\")\n\ndecoded_dummy = decode(encoded_dummy, loaded_vocab)\nprint(f\"Decoded sentence: {decoded_dummy}\")\n\nassert dummy_sentence == decoded_dummy, \"The tokenizer did not work correctly\"\nprint(\"All is good, God bless\")","metadata":{"id":"zwMA-6vHsJWB","executionInfo":{"status":"ok","timestamp":1756283138178,"user_tz":-180,"elapsed":44,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"e3776605-6a34-42ca-ccd1-dafd76df24e6","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:47.966308Z","iopub.execute_input":"2025-08-28T13:46:47.966788Z","iopub.status.idle":"2025-08-28T13:46:48.000423Z","shell.execute_reply.started":"2025-08-28T13:46:47.966766Z","shell.execute_reply":"2025-08-28T13:46:47.999685Z"}},"outputs":[{"name":"stdout","text":"Model file '/kaggle/input/modeldata/tokenizer.model' and vocab file '/kaggle/input/modeldata/tokenizer.vocab' already exist. Skipping training.\nLoading tokenizer\nTokenizer loaded\nChecking my tokenizer: \nOriginal sentence: I am confident my tokenizer works as it should\nEncoded sentence: [73, 615, 1163, 1368, 358, 281, 1265, 780, 266, 4157, 347, 324, 677]\nDecoded sentence: I am confident my tokenizer works as it should\nAll is good, God bless\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\ntorch.manual_seed(1337)\n\n#Hyperparameters\nblock_size = 256\nbatch_size = 32\nperiodicity_of_evaluation = 100\nlearning_rate = 1e-3 #maybe change it depends on how the model performs\nnum_max_iterations = 10000\nnumber_of_batches_to_eval = 200\ndropout = 0.2\nnum_embeddings = 256\nnum_heads = 6\nnum_layers = 6\n\n","metadata":{"id":"M_Nk87MRfGRL","executionInfo":{"status":"ok","timestamp":1756283138178,"user_tz":-180,"elapsed":6,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"18616d40-3182-4fc2-f7f9-699e2bc8f00c","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:48.001304Z","iopub.execute_input":"2025-08-28T13:46:48.001566Z","iopub.status.idle":"2025-08-28T13:46:48.007087Z","shell.execute_reply.started":"2025-08-28T13:46:48.001544Z","shell.execute_reply":"2025-08-28T13:46:48.006516Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"#In this cell I will save the new_data so I don't have to go again through the painstaking process of waiting for the whole dataset to be encoded\nENCODED_FILE = \"/kaggle/input/modeldata/encoded_text\"\nif os.path.exists(ENCODED_FILE):\n    print(f\"Encoded file '{ENCODED_FILE}' already exist. Skipping encoding.\")\n    new_data = torch.load(ENCODED_FILE)\nelse:\n  new_data = torch.tensor(encode(data,loaded_merges), dtype = torch.long)\n  torch.save(new_data,ENCODED_FILE)\n","metadata":{"id":"3ozikiDBvh3v","executionInfo":{"status":"ok","timestamp":1756283138219,"user_tz":-180,"elapsed":43,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"113225a0-0b39-4140-ee3d-8edf53d8d2ed","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:48.007654Z","iopub.execute_input":"2025-08-28T13:46:48.007852Z","iopub.status.idle":"2025-08-28T13:46:48.025034Z","shell.execute_reply.started":"2025-08-28T13:46:48.007838Z","shell.execute_reply":"2025-08-28T13:46:48.024528Z"}},"outputs":[{"name":"stdout","text":"Encoded file '/kaggle/input/modeldata/encoded_text' already exist. Skipping encoding.\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"print(len(new_data))\n# In this cell I will split the data but I will take chunks of size 20000 \n# so my training data and validation data are from every book\n# Previously I only took validation data from the last books so I believe my validation loss was measured inacuratelly\nchunk_size = 20000\nsplit_percentage = int(0.9*chunk_size)\ntrain_chunks = []\nvalidation_chunks = []\n\nfor i in range(0,len(new_data),chunk_size):\n    current_chunk = new_data[i:(i+chunk_size)]\n    train_chunks.append(current_chunk[:split_percentage])\n    validation_chunks.append(current_chunk[split_percentage:])\n\ntrain_data = torch.cat(train_chunks)\nvalidation_data = torch.cat(validation_chunks)\n\nprint(f\"Original data size: {len(new_data)}\")\nprint(f\"Training data size: {len(train_data)}\")\nprint(f\"Validation data size: {len(validation_data)}\")","metadata":{"id":"TF86NHXS46oL","executionInfo":{"status":"ok","timestamp":1756283138221,"user_tz":-180,"elapsed":1,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:48.025661Z","iopub.execute_input":"2025-08-28T13:46:48.025841Z","iopub.status.idle":"2025-08-28T13:46:48.036538Z","shell.execute_reply.started":"2025-08-28T13:46:48.025827Z","shell.execute_reply":"2025-08-28T13:46:48.035870Z"}},"outputs":[{"name":"stdout","text":"2284374\nOriginal data size: 2284374\nTraining data size: 2056374\nValidation data size: 228000\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"def get_batch(split):\n  inter_data = train_data if split == 'train' else validation_data\n  ix = torch.randint(len(inter_data) - block_size, (batch_size,))\n  x = torch.stack([inter_data[i:i+block_size] for i in ix])\n  y = torch.stack([inter_data[i+1:i+block_size+1] for i in ix])\n  x,y = x.to(device), y.to(device)\n  return x,y\n\n@torch.no_grad()\ndef approximate_loss():\n  out = {}\n  model.eval()\n  for split in ['train','val']:\n    losses = torch.zeros(number_of_batches_to_eval)\n    for k in range(number_of_batches_to_eval):\n      X,Y = get_batch(split)\n      logits, loss = model(X,Y)\n      losses[k] = loss.item()\n    out[split] = losses.mean()\n  model.train()\n  return out","metadata":{"id":"AEVYHL4l0L8v","executionInfo":{"status":"ok","timestamp":1756283138223,"user_tz":-180,"elapsed":1,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:48.037356Z","iopub.execute_input":"2025-08-28T13:46:48.037567Z","iopub.status.idle":"2025-08-28T13:46:48.044134Z","shell.execute_reply.started":"2025-08-28T13:46:48.037545Z","shell.execute_reply":"2025-08-28T13:46:48.043631Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"class Head(nn.Module):\n  def __init__(self, head_size):\n    super().__init__()\n    self.key = nn.Linear(num_embeddings, head_size, bias = False)\n    self.query = nn.Linear(num_embeddings, head_size, bias = False)\n    self.value = nn.Linear(num_embeddings, head_size, bias = False)\n    self.dropout = nn.Dropout(dropout)\n    self.register_buffer('mask', torch.tril(torch.ones(block_size,block_size)))\n  def forward(self, x): # x is the input\n    num_batches, time_steps, channels = x.shape\n    keys = self.key(x)\n    queries = self.query(x)\n    values = self.value(x)\n\n    weights = queries @ keys.transpose(-2,-1) * channels**-0.5 # weights will be of size (B,T,T)\n    weights = weights.masked_fill(self.mask[:time_steps,:time_steps] == 0, float('-inf'))\n    weights = F.softmax(weights, dim = -1)\n    weights = self.dropout(weights)\n    output = weights @ values #output will be of size (B,T,C)\n    return output\n\nclass MultiHead(nn.Module):\n  def __init__(self, num_heads, head_size):\n    super().__init__()\n    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n    self.projection = nn.Linear(num_heads * head_size, num_embeddings)\n    self.dropout = nn.Dropout(dropout)\n  def forward(self, x):\n    out = torch.cat([h(x) for h in self.heads], dim = -1)\n    out = self.dropout(self.projection(out))\n    return out\n\nclass FeedForward(nn.Module):\n  # This is a simple linear layer with a non-liner layer that follows it\n  def __init__(self, num_embeddings):\n    super().__init__()\n    self.net = nn.Sequential(\n        nn.Linear(num_embeddings, 4*num_embeddings),\n        nn.ReLU(),\n        nn.Linear(4*num_embeddings, num_embeddings),\n        nn.Dropout(dropout)\n    )\n  def forward(self, x):\n    return self.net(x)\n\n\nclass TransformerBlock(nn.Module):\n  # This is a whole transformer block\n  def __init__(self, num_embeddings, num_heads):\n    super().__init__()\n    head_size = num_embeddings // num_heads\n    self.multihead = MultiHead(num_heads, head_size)\n    self.feedforward = FeedForward(num_embeddings)\n    self.layernorm1 = nn.LayerNorm(num_embeddings)\n    self.layernorm2 = nn.LayerNorm(num_embeddings)\n\n  def forward(self, x):\n    x = x + self.multihead(self.layernorm1(x))\n    x = x + self.feedforward(self.layernorm2(x))\n    return x","metadata":{"id":"ZXGXXj20_R4I","executionInfo":{"status":"ok","timestamp":1756283138244,"user_tz":-180,"elapsed":2,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:48.045104Z","iopub.execute_input":"2025-08-28T13:46:48.045753Z","iopub.status.idle":"2025-08-28T13:46:48.058509Z","shell.execute_reply.started":"2025-08-28T13:46:48.045732Z","shell.execute_reply":"2025-08-28T13:46:48.057810Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"class MyLLM(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.token_embedding_table = nn.Embedding(VOCAB_SIZE, num_embeddings)\n    self.position_embedding_table = nn.Embedding(block_size, num_embeddings)\n    self.blocks = nn.Sequential(*[TransformerBlock(num_embeddings, num_heads) for _ in range(num_layers)])\n    self.layernorm = nn.LayerNorm(num_embeddings)\n    self.modeling_head = nn.Linear(num_embeddings, VOCAB_SIZE)\n\n  def forward(self,idx, targets = None):\n    token_emb = self.token_embedding_table(idx)\n    pos_emb = self.position_embedding_table(torch.arange(idx.shape[1], device = device))\n    x = token_emb + pos_emb\n    x = self.blocks(x)\n    x = self.layernorm(x)\n    logits = self.modeling_head(x)\n    if targets is None:\n      loss = None\n    else:\n      num_batches, time_steps, channels = logits.shape\n      logits = logits.view(num_batches*time_steps, channels)\n      targets = targets.view(num_batches*time_steps)\n      loss = F.cross_entropy(logits, targets)\n    return logits, loss\n  def generate(self, idx, max_new_tokens):\n    for _ in range(max_new_tokens):\n      idx_cond = idx[:, -block_size:] # these are the idx on which the generate will depend upon\n      logits, loss = self(idx_cond) # these are the predictions\n      logits = logits[:, -1, :] # only last prediction matters\n      probs = F.softmax(logits, dim = -1) # get the probabilities\n      idx_next = torch.multinomial(probs, num_samples = 1) # sample from the distribution so the token generated can be other than the one with maximum probability\n      idx = torch.cat((idx, idx_next), dim = 1) # append the new index to the running sequence\n    return idx\n","metadata":{"id":"3DvqmOyYR3p6","executionInfo":{"status":"ok","timestamp":1756283138295,"user_tz":-180,"elapsed":50,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:48.059573Z","iopub.execute_input":"2025-08-28T13:46:48.059776Z","iopub.status.idle":"2025-08-28T13:46:48.074668Z","shell.execute_reply.started":"2025-08-28T13:46:48.059762Z","shell.execute_reply":"2025-08-28T13:46:48.074028Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"model = MyLLM()\nm = model.to(device)\noptimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)\n\n# I will also add a decaying learning rate when the loss function plateaus \nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode = 'min',\n    factor = 0.1,\n    patience = 5,\n    threshold = 1e-3,\n    threshold_mode = 'rel'\n    \n)\n\nprint(f\"Initial learning rate: {learning_rate}\")\n\nfor iter in range(num_max_iterations):\n  if iter % periodicity_of_evaluation == 0 or iter == num_max_iterations-1:\n    losses = approximate_loss()\n    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n    scheduler.step(losses['val'])\n\n    current_lr = optimizer.param_groups[0]['lr']\n    print(f\"Current learning rate: {current_lr}\")\n    \n  X,Y = get_batch('train')\n\n  logits, loss = model(X,Y)\n  optimizer.zero_grad(set_to_none = True)\n  loss.backward()\n  optimizer.step()\n\ncontext = torch.zeros((1,1),dtype = torch.long, device = device)\nprint(decode(m.generate(context, max_new_tokens = 500)[0].tolist(), loaded_vocab))","metadata":{"id":"gr6wA1UllsJY","executionInfo":{"status":"error","timestamp":1756284233116,"user_tz":-180,"elapsed":5508,"user":{"displayName":"Luca Tasadan","userId":"10296113240313538113"}},"outputId":"842082dc-02b9-4510-e90f-afacb04bb033","trusted":true,"execution":{"iopub.status.busy":"2025-08-28T13:46:53.464832Z","iopub.execute_input":"2025-08-28T13:46:53.465606Z","iopub.status.idle":"2025-08-28T15:07:43.082429Z","shell.execute_reply.started":"2025-08-28T13:46:53.465570Z","shell.execute_reply":"2025-08-28T15:07:43.081788Z"}},"outputs":[{"name":"stdout","text":"Initial learning rate: 0.001\nstep 0: train loss 9.1904, val loss 9.1898\nCurrent learning rate: 0.001\nstep 100: train loss 6.2311, val loss 6.2404\nCurrent learning rate: 0.001\nstep 200: train loss 5.7415, val loss 5.7800\nCurrent learning rate: 0.001\nstep 300: train loss 5.4799, val loss 5.5366\nCurrent learning rate: 0.001\nstep 400: train loss 5.2291, val loss 5.3147\nCurrent learning rate: 0.001\nstep 500: train loss 5.0509, val loss 5.1566\nCurrent learning rate: 0.001\nstep 600: train loss 4.9227, val loss 5.0504\nCurrent learning rate: 0.001\nstep 700: train loss 4.8033, val loss 4.9504\nCurrent learning rate: 0.001\nstep 800: train loss 4.7232, val loss 4.8779\nCurrent learning rate: 0.001\nstep 900: train loss 4.6486, val loss 4.8258\nCurrent learning rate: 0.001\nstep 1000: train loss 4.5855, val loss 4.7890\nCurrent learning rate: 0.001\nstep 1100: train loss 4.5242, val loss 4.7351\nCurrent learning rate: 0.001\nstep 1200: train loss 4.4770, val loss 4.7153\nCurrent learning rate: 0.001\nstep 1300: train loss 4.4285, val loss 4.6720\nCurrent learning rate: 0.001\nstep 1400: train loss 4.3823, val loss 4.6474\nCurrent learning rate: 0.001\nstep 1500: train loss 4.3510, val loss 4.6176\nCurrent learning rate: 0.001\nstep 1600: train loss 4.2927, val loss 4.5878\nCurrent learning rate: 0.001\nstep 1700: train loss 4.2644, val loss 4.5670\nCurrent learning rate: 0.001\nstep 1800: train loss 4.2228, val loss 4.5440\nCurrent learning rate: 0.001\nstep 1900: train loss 4.1969, val loss 4.5270\nCurrent learning rate: 0.001\nstep 2000: train loss 4.1711, val loss 4.5051\nCurrent learning rate: 0.001\nstep 2100: train loss 4.1399, val loss 4.4978\nCurrent learning rate: 0.001\nstep 2200: train loss 4.1100, val loss 4.4975\nCurrent learning rate: 0.001\nstep 2300: train loss 4.0841, val loss 4.4678\nCurrent learning rate: 0.001\nstep 2400: train loss 4.0516, val loss 4.4578\nCurrent learning rate: 0.001\nstep 2500: train loss 4.0255, val loss 4.4385\nCurrent learning rate: 0.001\nstep 2600: train loss 4.0111, val loss 4.4359\nCurrent learning rate: 0.001\nstep 2700: train loss 3.9894, val loss 4.4180\nCurrent learning rate: 0.001\nstep 2800: train loss 3.9701, val loss 4.4151\nCurrent learning rate: 0.001\nstep 2900: train loss 3.9405, val loss 4.4061\nCurrent learning rate: 0.001\nstep 3000: train loss 3.9240, val loss 4.3919\nCurrent learning rate: 0.001\nstep 3100: train loss 3.9032, val loss 4.3932\nCurrent learning rate: 0.001\nstep 3200: train loss 3.8979, val loss 4.3858\nCurrent learning rate: 0.001\nstep 3300: train loss 3.8613, val loss 4.3746\nCurrent learning rate: 0.001\nstep 3400: train loss 3.8572, val loss 4.3747\nCurrent learning rate: 0.001\nstep 3500: train loss 3.8296, val loss 4.3608\nCurrent learning rate: 0.001\nstep 3600: train loss 3.8127, val loss 4.3447\nCurrent learning rate: 0.001\nstep 3700: train loss 3.7930, val loss 4.3528\nCurrent learning rate: 0.001\nstep 3800: train loss 3.7842, val loss 4.3451\nCurrent learning rate: 0.001\nstep 3900: train loss 3.7671, val loss 4.3381\nCurrent learning rate: 0.001\nstep 4000: train loss 3.7639, val loss 4.3337\nCurrent learning rate: 0.001\nstep 4100: train loss 3.7364, val loss 4.3274\nCurrent learning rate: 0.001\nstep 4200: train loss 3.7218, val loss 4.3224\nCurrent learning rate: 0.001\nstep 4300: train loss 3.7128, val loss 4.3224\nCurrent learning rate: 0.001\nstep 4400: train loss 3.6920, val loss 4.3218\nCurrent learning rate: 0.001\nstep 4500: train loss 3.6777, val loss 4.3130\nCurrent learning rate: 0.001\nstep 4600: train loss 3.6721, val loss 4.3118\nCurrent learning rate: 0.001\nstep 4700: train loss 3.6633, val loss 4.3135\nCurrent learning rate: 0.001\nstep 4800: train loss 3.6345, val loss 4.3161\nCurrent learning rate: 0.001\nstep 4900: train loss 3.6421, val loss 4.3183\nCurrent learning rate: 0.001\nstep 5000: train loss 3.6140, val loss 4.3105\nCurrent learning rate: 0.001\nstep 5100: train loss 3.6141, val loss 4.3144\nCurrent learning rate: 0.0001\nstep 5200: train loss 3.5447, val loss 4.2612\nCurrent learning rate: 0.0001\nstep 5300: train loss 3.5243, val loss 4.2462\nCurrent learning rate: 0.0001\nstep 5400: train loss 3.5182, val loss 4.2363\nCurrent learning rate: 0.0001\nstep 5500: train loss 3.5009, val loss 4.2426\nCurrent learning rate: 0.0001\nstep 5600: train loss 3.4951, val loss 4.2336\nCurrent learning rate: 0.0001\nstep 5700: train loss 3.4918, val loss 4.2267\nCurrent learning rate: 0.0001\nstep 5800: train loss 3.4794, val loss 4.2307\nCurrent learning rate: 0.0001\nstep 5900: train loss 3.4716, val loss 4.2256\nCurrent learning rate: 0.0001\nstep 6000: train loss 3.4694, val loss 4.2253\nCurrent learning rate: 0.0001\nstep 6100: train loss 3.4659, val loss 4.2363\nCurrent learning rate: 0.0001\nstep 6200: train loss 3.4646, val loss 4.2202\nCurrent learning rate: 0.0001\nstep 6300: train loss 3.4558, val loss 4.2090\nCurrent learning rate: 0.0001\nstep 6400: train loss 3.4514, val loss 4.2084\nCurrent learning rate: 0.0001\nstep 6500: train loss 3.4440, val loss 4.2140\nCurrent learning rate: 0.0001\nstep 6600: train loss 3.4412, val loss 4.2170\nCurrent learning rate: 0.0001\nstep 6700: train loss 3.4474, val loss 4.2171\nCurrent learning rate: 0.0001\nstep 6800: train loss 3.4369, val loss 4.2155\nCurrent learning rate: 0.0001\nstep 6900: train loss 3.4356, val loss 4.2253\nCurrent learning rate: 1e-05\nstep 7000: train loss 3.4343, val loss 4.2055\nCurrent learning rate: 1e-05\nstep 7100: train loss 3.4306, val loss 4.2083\nCurrent learning rate: 1e-05\nstep 7200: train loss 3.4326, val loss 4.2080\nCurrent learning rate: 1e-05\nstep 7300: train loss 3.4320, val loss 4.2142\nCurrent learning rate: 1e-05\nstep 7400: train loss 3.4216, val loss 4.2121\nCurrent learning rate: 1e-05\nstep 7500: train loss 3.4328, val loss 4.2119\nCurrent learning rate: 1.0000000000000002e-06\nstep 7600: train loss 3.4222, val loss 4.2116\nCurrent learning rate: 1.0000000000000002e-06\nstep 7700: train loss 3.4289, val loss 4.1998\nCurrent learning rate: 1.0000000000000002e-06\nstep 7800: train loss 3.4312, val loss 4.2125\nCurrent learning rate: 1.0000000000000002e-06\nstep 7900: train loss 3.4221, val loss 4.2058\nCurrent learning rate: 1.0000000000000002e-06\nstep 8000: train loss 3.4201, val loss 4.2198\nCurrent learning rate: 1.0000000000000002e-06\nstep 8100: train loss 3.4237, val loss 4.2041\nCurrent learning rate: 1.0000000000000002e-06\nstep 8200: train loss 3.4210, val loss 4.2078\nCurrent learning rate: 1.0000000000000002e-06\nstep 8300: train loss 3.4230, val loss 4.2021\nCurrent learning rate: 1.0000000000000002e-07\nstep 8400: train loss 3.4303, val loss 4.1968\nCurrent learning rate: 1.0000000000000002e-07\nstep 8500: train loss 3.4206, val loss 4.2106\nCurrent learning rate: 1.0000000000000002e-07\nstep 8600: train loss 3.4215, val loss 4.2234\nCurrent learning rate: 1.0000000000000002e-07\nstep 8700: train loss 3.4182, val loss 4.2145\nCurrent learning rate: 1.0000000000000002e-07\nstep 8800: train loss 3.4234, val loss 4.1978\nCurrent learning rate: 1.0000000000000002e-07\nstep 8900: train loss 3.4254, val loss 4.2114\nCurrent learning rate: 1.0000000000000004e-08\nstep 9000: train loss 3.4167, val loss 4.2060\nCurrent learning rate: 1.0000000000000004e-08\nstep 9100: train loss 3.4202, val loss 4.2183\nCurrent learning rate: 1.0000000000000004e-08\nstep 9200: train loss 3.4216, val loss 4.2152\nCurrent learning rate: 1.0000000000000004e-08\nstep 9300: train loss 3.4250, val loss 4.2182\nCurrent learning rate: 1.0000000000000004e-08\nstep 9400: train loss 3.4161, val loss 4.2109\nCurrent learning rate: 1.0000000000000004e-08\nstep 9500: train loss 3.4206, val loss 4.2096\nCurrent learning rate: 1.0000000000000004e-08\nstep 9600: train loss 3.4265, val loss 4.2115\nCurrent learning rate: 1.0000000000000004e-08\nstep 9700: train loss 3.4304, val loss 4.2080\nCurrent learning rate: 1.0000000000000004e-08\nstep 9800: train loss 3.4247, val loss 4.2105\nCurrent learning rate: 1.0000000000000004e-08\nstep 9900: train loss 3.4314, val loss 4.2088\nCurrent learning rate: 1.0000000000000004e-08\nstep 9999: train loss 3.4248, val loss 4.2052\nCurrent learning rate: 1.0000000000000004e-08\n\u0000 of the sky and rolled, into unreal space, and showing\nthe space of the twilight of applause soon arose, as if\nMrs Musgrove was fulfilling. Someone reader her husband like her loads’\nbottles, who had been rather wrath heraldness, had met\nstones sailors.\n\n“Do tell her that her feelings of meeting she _have grown\nliver of the estimments;_ such generally, such a thing as a soul so fond\na ruscript; and she showed well all the names “adridged. Where a gentleman died away\nfrom the country of great American language! I did not\nit be added to give her to the idea of a woman of the blessing;\nand endeavouring, if it’d come forward to rest intently all the\nLouisa of it because they were in their way and from them.\n\n  [7]  I dwell yesterday; Nurse Hascal upon, that I’m\nstill a pity I just little thought.\n\nYou will tell me about.”\n\nI was finished.\n\nHe looked at my own map, and he smiled for a moment. I saw a second feeling\nof emotion from the agitated expression abruptly at a circle, and then turning\ncrushed every half, nearer.\n\nShe held. She boded as if she is clearly finished by her, but he\nwas engaged in the air. The mood put her face as if he serender\nwould be quiet calm and invisible. Weaking Mr Elliot’s rest\ndelight very distress. That, however, he went into some ear with an\nthereafternoon, but I was in mind to go to my room, rather\nhelpless this, and sought what happened a citation, first, and the\nmaking of his family.\n\n“‘Of course, if you got away it, certainly required is an irresistible\ndisadvantage to it;” asked the balls he, “that is the only big\ntrue.”\n\n_Sphere_. I passed outside the quiet air behind ground I had left to stop. It\nwas destined to be the faintest sensations of a wellangle. And a\nyoupect! the past you,-up? I say, I heard you a man know\nthe fit.”\n\n“You come with him, if in a minute, I suppose, you shall find there know\non your tree?”\n\n“How?”\n\nHis visitor lost his head. He could\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"\nprint(\"Training finished. Saving model...\")\ntorch.save(model.state_dict(), 'model_weights2.pth')\nprint(\"Model saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T15:09:26.387231Z","iopub.execute_input":"2025-08-28T15:09:26.387983Z","iopub.status.idle":"2025-08-28T15:09:26.516659Z","shell.execute_reply.started":"2025-08-28T15:09:26.387956Z","shell.execute_reply":"2025-08-28T15:09:26.515892Z"}},"outputs":[{"name":"stdout","text":"Training finished. Saving model...\nModel saved!\n","output_type":"stream"}],"execution_count":97}]}